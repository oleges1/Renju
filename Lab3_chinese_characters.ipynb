{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "translator = dict()\n",
    "retranslator = np.empty([1000])\n",
    "temp = 0\n",
    "\n",
    "def preparing_img(img):\n",
    "    return resize(img, (64, 64), mode = 'reflect')\n",
    "\n",
    "def preparing_label(label):\n",
    "    global temp\n",
    "    if (translator.get(label) == None):\n",
    "        translator[label] = temp\n",
    "        retranslator[temp] = label\n",
    "        temp += 1\n",
    "    label = translator[label]\n",
    "    res = np.zeros([1000])\n",
    "    res[label] = 1\n",
    "    return res\n",
    "        \n",
    "def preparing_test(test):\n",
    "    res = np.empty([83247, 64, 64])\n",
    "    for i in range(len(test)):\n",
    "        res[i] = resize(test[i], [64, 64], mode = 'reflect')\n",
    "        test[i] = np.empty([0])\n",
    "    test = np.empty([0])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for train_num in range(1, 5):\n",
    "    data_name = 'data/train-' + str(train_num) + '.npy'\n",
    "    data = np.load(data_name)\n",
    "    imgs = data.T[0]\n",
    "    labels = data.T[1]\n",
    "    data = np.empty([0])\n",
    "    np.save('data/imgs-' + str(train_num) + '.npy', np.array([preparing_img(img) for img in imgs]))\n",
    "    imgs = np.empty([0])\n",
    "    np.save('data/labels-' + str(train_num) + '.npy', np.array([preparing_label(label) for label in labels]))\n",
    "    labels = np.empty([0])\n",
    "    print('.')\n",
    "    \n",
    "np.save('data/retranslator.npy', retranslator)\n",
    "test = np.load('data/test.npy')\n",
    "np.save('data/pretest.npy', np.array([preparing_img(img) for img in test]))\n",
    "test = np.empty([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    '''for i in range(len(labels_shuffle)):\n",
    "        place = labels_shuffle[i]\n",
    "        labels_shuffle[i] = np.zeros(1000)\n",
    "        labels_shuffle[i][place] = 1'''\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "l = 0.1\n",
    "def leaky_relu(features):\n",
    "    return (1 - l) * tf.nn.relu(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "l1, l2 = exp(-10), exp(-10)\n",
    "def gradient_descent(shapes, nonlinearity):\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 4320])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 1000])\n",
    "    \n",
    "    list_of_layers = [x]\n",
    "    for i in range(1, len(shapes) - 1):\n",
    "        list_of_layers.append(\n",
    "            tf.layers.dense(\n",
    "                    inputs=list_of_layers[i-1],\n",
    "                    units=shapes[i],\n",
    "                    activation=nonlinearity,\n",
    "                    use_bias=True,\n",
    "                    kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(l1, l2)\n",
    "                ))\n",
    "\n",
    "    logits = tf.layers.dense(\n",
    "                    inputs=list_of_layers[-1],\n",
    "                    units=shapes[-1],\n",
    "                    use_bias=True,\n",
    "                    kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(l1, l2)\n",
    "                )\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits = logits)) + tf.losses.get_regularization_loss()\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    \n",
    "    prediction = tf.argmax(logits,1)\n",
    "    correct_prediction = tf.equal(prediction, tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for train_num in range(1, 5):\n",
    "            data_name = 'data/train-' + str(train_num) + '.npy'\n",
    "            data = np.load(data_name)\n",
    "            preparing(data)\n",
    "            for i in range(12000):\n",
    "                batch = next_batch(100, data.T[0], data.T[1])\n",
    "                train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "                if (i % 500 == 0):\n",
    "                    print('.', end = '')\n",
    "            print('#')\n",
    "            batch = next_batch(3000, data.T[0], data.T[1])\n",
    "            data = np.empty([0])\n",
    "            print(\"accuracy:\", accuracy.eval(feed_dict={x: batch[0], y_: batch[1]}))\n",
    "            batch = np.empty([0])\n",
    "        test = np.load('data/test.npy')\n",
    "        test.reshape(83247, 1)\n",
    "        test = preparing_test(test)\n",
    "        print(\"test ready, now session\")\n",
    "        return sess.run(prediction, feed_dict = {x: test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "start = time()\n",
    "res = gradient_descent([4096, 2548, 1774, 1000], leaky_relu)\n",
    "print(\"time:\", time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Id', 'Category'])\n",
    "for i in range(len(res)):\n",
    "    df.loc[i] = [i + 1, retranslator(res[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOTHER NEURAL NETWORK JUST COPY OF DEEP TUTORIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def gradient_descent_perfecto():\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 72, 60])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 1000])\n",
    "    \n",
    "    #first conv\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    \n",
    "    x_image = tf.reshape(x, [-1, 72, 60, 1])\n",
    "\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    # second conv\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    W_fc1 = weight_variable([18 * 15 * 64, 6144])\n",
    "    b_fc1 = bias_variable([6144])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 18 * 15 * 64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    W_fc2 = weight_variable([6144, 1000])\n",
    "    b_fc2 = bias_variable([1000])\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    prediction = tf.argmax(y_conv, 1)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for train_num in range(1, 5):\n",
    "            data_name = 'data/train-' + str(train_num) + '.npy'\n",
    "            data = np.load(data_name)\n",
    "            preparing(data)\n",
    "            for i in range(20000):\n",
    "                batch = next_batch(100, data.T[0], data.T[1])\n",
    "                train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "                if (i % 500 == 0):\n",
    "                    print('.', end = '')\n",
    "            print('#')\n",
    "            batch = next_batch(3000, data.T[0], data.T[1])\n",
    "            data = np.empty([0])\n",
    "            print(\"accuracy:\", accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0}))\n",
    "            batch = np.empty([0])\n",
    "        test = np.load('data/test.npy')\n",
    "        test.reshape(83247, 1)\n",
    "        test = preparing_test(test)\n",
    "        print(\"test ready, now session\")\n",
    "        return sess.run(prediction, feed_dict = {x: test, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[17280,6144]\n\t [[Node: Variable_4/Adam_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_4\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_4/Adam_1, Variable_4/Adam/Initializer/zeros)]]\n\nCaused by op 'Variable_4/Adam_1/Assign', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-8659f5a3fb53>\", line 6, in <module>\n    res = gradient_descent_perfecto()\n  File \"<ipython-input-9-416bc1f192c7>\", line 42, in gradient_descent_perfecto\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 353, in minimize\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 474, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/adam.py\", line 137, in _create_slots\n    self._zeros_slot(v, \"v\", self._name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 796, in _zeros_slot\n    named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/slot_creator.py\", line 148, in create_slot_with_initializer\n    dtype)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/slot_creator.py\", line 67, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1203, in get_variable\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1092, in get_variable\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 805, in _get_single_variable\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 346, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 57, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[17280,6144]\n\t [[Node: Variable_4/Adam_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_4\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_4/Adam_1, Variable_4/Adam/Initializer/zeros)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[17280,6144]\n\t [[Node: Variable_4/Adam_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_4\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_4/Adam_1, Variable_4/Adam/Initializer/zeros)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8659f5a3fb53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_perfecto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-416bc1f192c7>\u001b[0m in \u001b[0;36mgradient_descent_perfecto\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mdata_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/train-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[17280,6144]\n\t [[Node: Variable_4/Adam_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_4\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_4/Adam_1, Variable_4/Adam/Initializer/zeros)]]\n\nCaused by op 'Variable_4/Adam_1/Assign', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-8659f5a3fb53>\", line 6, in <module>\n    res = gradient_descent_perfecto()\n  File \"<ipython-input-9-416bc1f192c7>\", line 42, in gradient_descent_perfecto\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 353, in minimize\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 474, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/adam.py\", line 137, in _create_slots\n    self._zeros_slot(v, \"v\", self._name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 796, in _zeros_slot\n    named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/slot_creator.py\", line 148, in create_slot_with_initializer\n    dtype)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/slot_creator.py\", line 67, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1203, in get_variable\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1092, in get_variable\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 805, in _get_single_variable\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 346, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 57, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[17280,6144]\n\t [[Node: Variable_4/Adam_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_4\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_4/Adam_1, Variable_4/Adam/Initializer/zeros)]]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "start = time()\n",
    "res = gradient_descent_perfecto()\n",
    "print(\"time:\", time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Id', 'Category'])\n",
    "for i in range(len(res)):\n",
    "    df.loc[i] = [i + 1, retranslator(res[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def IMDG(x_train):\n",
    "    datagen = ImageDataGenerator(\n",
    "            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.03,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.03)  # randomly shift images vertically (fraction of total height)\n",
    "    return datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saver(path, res):\n",
    "    retranslator = np.load('data/retranslator.npy')\n",
    "    with open(path, 'w') as out:\n",
    "        print('Id,Category', file=out)\n",
    "        for i in range(len(res)):\n",
    "            print('{i},{res}'.format(i = i + 1, res=int(retranslator[np.argmax(res[i])])), file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 64, 64, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 32, 32, 64)    3200        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 16, 16, 64)    0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 16, 16, 64)    36928       max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 16, 16, 64)    256         conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)        (None, 16, 16, 64)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 16, 16, 64)    0           leaky_re_lu_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 16, 16, 64)    36928       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 16, 16, 64)    0           conv2d_3[0][0]                   \n",
      "                                                                   max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 16, 16, 64)    256         add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)        (None, 16, 16, 64)    0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 16, 16, 64)    0           leaky_re_lu_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 16, 16, 64)    36928       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 16, 16, 64)    256         conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)        (None, 16, 16, 64)    0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 16, 16, 64)    0           leaky_re_lu_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 16, 16, 64)    36928       dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 16, 16, 64)    0           conv2d_5[0][0]                   \n",
      "                                                                   dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 16, 16, 64)    256         add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)        (None, 16, 16, 64)    0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 16, 16, 64)    0           leaky_re_lu_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 16, 16, 64)    36928       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 16, 16, 64)    256         conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)        (None, 16, 16, 64)    0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 16, 16, 64)    0           leaky_re_lu_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 16, 16, 64)    36928       dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 16, 16, 64)    0           conv2d_7[0][0]                   \n",
      "                                                                   dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 16, 16, 64)    256         add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)        (None, 16, 16, 64)    0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 16, 16, 64)    0           leaky_re_lu_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 8, 8, 128)     73856       dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 8, 8, 128)     147584      conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 8, 8, 128)     512         conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)        (None, 8, 8, 128)     0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 8, 8, 128)     0           leaky_re_lu_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 8, 8, 128)     147584      dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 8, 8, 128)     0           conv2d_10[0][0]                  \n",
      "                                                                   conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 8, 8, 128)     512         add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)        (None, 8, 8, 128)     0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 8, 8, 128)     0           leaky_re_lu_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 8, 8, 128)     147584      dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 8, 8, 128)     512         conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)        (None, 8, 8, 128)     0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 8, 8, 128)     0           leaky_re_lu_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 8, 8, 128)     147584      dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 8, 8, 128)     0           conv2d_12[0][0]                  \n",
      "                                                                   dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 8, 8, 128)     512         add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)       (None, 8, 8, 128)     0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 8, 8, 128)     0           leaky_re_lu_10[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 8, 8, 128)     147584      dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 8, 8, 128)     512         conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)       (None, 8, 8, 128)     0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 8, 8, 128)     0           leaky_re_lu_11[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 8, 8, 128)     147584      dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 8, 8, 128)     0           conv2d_14[0][0]                  \n",
      "                                                                   dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 8, 8, 128)     512         add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)       (None, 8, 8, 128)     0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 8, 8, 128)     0           leaky_re_lu_12[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 4, 4, 256)     295168      dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)               (None, 4, 4, 256)     590080      conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 4, 4, 256)     1024        conv2d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)       (None, 4, 4, 256)     0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 4, 4, 256)     0           leaky_re_lu_13[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)               (None, 4, 4, 256)     590080      dropout_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      (None, 4, 4, 256)     0           conv2d_17[0][0]                  \n",
      "                                                                   conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 4, 4, 256)     1024        add_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)       (None, 4, 4, 256)     0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 4, 4, 256)     0           leaky_re_lu_14[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)               (None, 4, 4, 256)     590080      dropout_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 4, 4, 256)     1024        conv2d_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)       (None, 4, 4, 256)     0           batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 4, 4, 256)     0           leaky_re_lu_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)               (None, 4, 4, 256)     590080      dropout_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      (None, 4, 4, 256)     0           conv2d_19[0][0]                  \n",
      "                                                                   dropout_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 4, 4, 256)     1024        add_8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)       (None, 4, 4, 256)     0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 4, 4, 256)     0           leaky_re_lu_16[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)               (None, 4, 4, 256)     590080      dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 4, 4, 256)     1024        conv2d_20[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)       (None, 4, 4, 256)     0           batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)             (None, 4, 4, 256)     0           leaky_re_lu_17[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)               (None, 4, 4, 256)     590080      dropout_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_9 (Add)                      (None, 4, 4, 256)     0           conv2d_21[0][0]                  \n",
      "                                                                   dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, 4, 4, 256)     1024        add_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)       (None, 4, 4, 256)     0           batch_normalization_18[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 4, 4, 256)     0           leaky_re_lu_18[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)               (None, 4, 4, 256)     590080      dropout_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNor (None, 4, 4, 256)     1024        conv2d_22[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)       (None, 4, 4, 256)     0           batch_normalization_19[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)             (None, 4, 4, 256)     0           leaky_re_lu_19[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)               (None, 4, 4, 256)     590080      dropout_19[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_10 (Add)                     (None, 4, 4, 256)     0           conv2d_23[0][0]                  \n",
      "                                                                   dropout_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNor (None, 4, 4, 256)     1024        add_10[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)       (None, 4, 4, 256)     0           batch_normalization_20[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)             (None, 4, 4, 256)     0           leaky_re_lu_20[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)               (None, 4, 4, 256)     590080      dropout_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNor (None, 4, 4, 256)     1024        conv2d_24[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)       (None, 4, 4, 256)     0           batch_normalization_21[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)             (None, 4, 4, 256)     0           leaky_re_lu_21[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)               (None, 4, 4, 256)     590080      dropout_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_11 (Add)                     (None, 4, 4, 256)     0           conv2d_25[0][0]                  \n",
      "                                                                   dropout_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNor (None, 4, 4, 256)     1024        add_11[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)       (None, 4, 4, 256)     0           batch_normalization_22[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)             (None, 4, 4, 256)     0           leaky_re_lu_22[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePool (None, 2, 2, 256)     0           dropout_22[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1024)          0           average_pooling2d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1000)          1025000     flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 8,419,944\n",
      "Trainable params: 8,412,520\n",
      "Non-trainable params: 7,424\n",
      "____________________________________________________________________________________________________\n",
      "1 from 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1207/1206 [==============================] - 587s - loss: 4.5843 - acc: 0.1374   \n",
      "Epoch 2/2\n",
      "1207/1206 [==============================] - 558s - loss: 1.5370 - acc: 0.5885   \n",
      "20000/20000 [==============================] - 37s    \n",
      "[1.232869561612606, 0.67335000365972519]\n",
      "2 from 4\n",
      "Epoch 1/2\n",
      "1207/1206 [==============================] - 558s - loss: 1.0439 - acc: 0.7171   \n",
      "Epoch 2/2\n",
      "1207/1206 [==============================] - 555s - loss: 0.7410 - acc: 0.7932   \n",
      "20000/20000 [==============================] - 36s    \n",
      "[3.8966691255569459, 0.28420000046491622]\n",
      "3 from 4\n",
      "Epoch 1/2\n",
      "1207/1206 [==============================] - 574s - loss: 0.6129 - acc: 0.8254   \n",
      "Epoch 2/2\n",
      "1207/1206 [==============================] - 555s - loss: 0.5046 - acc: 0.8546   \n",
      "20000/20000 [==============================] - 37s    \n",
      "[0.53421889148652557, 0.85149999737739568]\n",
      "4 from 4\n",
      "Epoch 1/2\n",
      "1207/1206 [==============================] - 558s - loss: 0.5225 - acc: 0.8508   \n",
      "Epoch 2/2\n",
      "1207/1206 [==============================] - 557s - loss: 0.4397 - acc: 0.8728   \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ca957b3d444f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, LeakyReLU, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "\n",
    "x = Input(shape=(64, 64, 1))\n",
    "\n",
    "conv = Conv2D(64, kernel_size=(7, 7), padding = 'same', strides = 2)(x)\n",
    "m_pool = MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "\n",
    "conv_1 = Conv2D(64, kernel_size=(3, 3), padding = 'same')(m_pool)\n",
    "batch_n_1 = BatchNormalization(axis = 3)(conv_1)\n",
    "leaky_1 = LeakyReLU()(batch_n_1)\n",
    "drop_1 = Dropout(0.25)(leaky_1)\n",
    "conv_1_2 = Conv2D(64, kernel_size=(3, 3), padding = 'same')(drop_1)\n",
    "res_1 = keras.layers.add([conv_1_2, m_pool])\n",
    "batch_n_1_2 = BatchNormalization(axis = 3)(res_1)\n",
    "leaky_1_2 = LeakyReLU()(batch_n_1_2)\n",
    "drop_1_2 = Dropout(0.25)(leaky_1_2)\n",
    "\n",
    "conv_2 = Conv2D(64, kernel_size=(3, 3), padding = 'same')(drop_1_2)\n",
    "batch_n_2 = BatchNormalization(axis = 3)(conv_2)\n",
    "leaky_2 = LeakyReLU()(batch_n_2)\n",
    "drop_2 = Dropout(0.25)(leaky_2)\n",
    "conv_2_2 = Conv2D(64, kernel_size=(3, 3), padding = 'same')(drop_2)\n",
    "res_2 = keras.layers.add([conv_2_2, drop_1_2])\n",
    "batch_n_2_2 = BatchNormalization(axis = 3)(res_2)\n",
    "leaky_2_2 = LeakyReLU()(batch_n_2_2)\n",
    "drop_2_2 = Dropout(0.25)(leaky_2_2)\n",
    "\n",
    "conv_3 = Conv2D(64, kernel_size=(3, 3), padding = 'same')(drop_2_2)\n",
    "batch_n_3 = BatchNormalization(axis = 3)(conv_3)\n",
    "leaky_3 = LeakyReLU()(batch_n_3)\n",
    "drop_3 = Dropout(0.25)(leaky_3)\n",
    "conv_3_2 = Conv2D(64, kernel_size=(3, 3), padding = 'same')(drop_3)\n",
    "res_3 = keras.layers.add([conv_3_2, drop_2_2])\n",
    "batch_n_3_2 = BatchNormalization(axis = 3)(res_3)\n",
    "leaky_3_2 = LeakyReLU()(batch_n_3_2)\n",
    "drop_3_2 = Dropout(0.25)(leaky_3_2)\n",
    "\n",
    "conv_128 = Conv2D(128, kernel_size=(3, 3), padding = 'same', strides = 2)(drop_3_2)\n",
    "\n",
    "conv_4 = Conv2D(128, kernel_size=(3, 3), padding = 'same')(conv_128)\n",
    "batch_n_4 = BatchNormalization(axis = 3)(conv_4)\n",
    "leaky_4 = LeakyReLU()(batch_n_4)\n",
    "drop_4 = Dropout(0.25)(leaky_4)\n",
    "conv_4_2 = Conv2D(128, kernel_size=(3, 3), padding = 'same')(drop_4)\n",
    "res_4 = keras.layers.add([conv_4_2, conv_128])\n",
    "batch_n_4_2 = BatchNormalization(axis = 3)(res_4)\n",
    "leaky_4_2 = LeakyReLU()(batch_n_4_2)\n",
    "drop_4_2 = Dropout(0.25)(leaky_4_2)\n",
    "\n",
    "conv_5 = Conv2D(128, kernel_size=(3, 3), padding = 'same')(drop_4_2)\n",
    "batch_n_5 = BatchNormalization(axis = 3)(conv_5)\n",
    "leaky_5 = LeakyReLU()(batch_n_5)\n",
    "drop_5 = Dropout(0.25)(leaky_5)\n",
    "conv_5_2 = Conv2D(128, kernel_size=(3, 3), padding = 'same')(drop_5)\n",
    "res_5 = keras.layers.add([conv_5_2, drop_4_2])\n",
    "batch_n_5_2 = BatchNormalization(axis = 3)(res_5)\n",
    "leaky_5_2 = LeakyReLU()(batch_n_5_2)\n",
    "drop_5_2 = Dropout(0.25)(leaky_5_2)\n",
    "\n",
    "conv_6 = Conv2D(128, kernel_size=(3, 3), padding = 'same')(drop_5_2)\n",
    "batch_n_6 = BatchNormalization(axis = 3)(conv_6)\n",
    "leaky_6 = LeakyReLU()(batch_n_6)\n",
    "drop_6 = Dropout(0.25)(leaky_6)\n",
    "conv_6_2 = Conv2D(128, kernel_size=(3, 3), padding = 'same')(drop_6)\n",
    "res_6 = keras.layers.add([conv_6_2, drop_5_2])\n",
    "batch_n_6_2 = BatchNormalization(axis = 3)(res_6)\n",
    "leaky_6_2 = LeakyReLU()(batch_n_6_2)\n",
    "drop_6_2 = Dropout(0.25)(leaky_6_2)\n",
    "\n",
    "conv_256 = Conv2D(256, kernel_size=(3, 3), padding = 'same', strides = 2)(drop_6_2)\n",
    "\n",
    "conv_7 = Conv2D(256, kernel_size=(3, 3), padding = 'same')(conv_256)\n",
    "batch_n_7 = BatchNormalization(axis = 3)(conv_7)\n",
    "leaky_7 = LeakyReLU()(batch_n_7)\n",
    "drop_7 = Dropout(0.25)(leaky_7)\n",
    "conv_7_2 = Conv2D(256, kernel_size=(3, 3), padding = 'same')(drop_7)\n",
    "res_7 = keras.layers.add([conv_7_2, conv_256])\n",
    "batch_n_7_2 = BatchNormalization(axis = 3)(res_7)\n",
    "leaky_7_2 = LeakyReLU()(batch_n_7_2)\n",
    "drop_7_2 = Dropout(0.25)(leaky_7_2)\n",
    "\n",
    "conv_8 = Conv2D(256, kernel_size=(3, 3), padding = 'same')(drop_7_2)\n",
    "batch_n_8 = BatchNormalization(axis = 3)(conv_8)\n",
    "leaky_8 = LeakyReLU()(batch_n_8)\n",
    "drop_8 = Dropout(0.25)(leaky_8)\n",
    "conv_8_2 = Conv2D(256, kernel_size=(3, 3), padding = 'same')(drop_8)\n",
    "res_8 = keras.layers.add([conv_8_2, drop_7_2])\n",
    "batch_n_8_2 = BatchNormalization(axis = 3)(res_8)\n",
    "leaky_8_2 = LeakyReLU()(batch_n_8_2)\n",
    "drop_8_2 = Dropout(0.25)(leaky_8_2)\n",
    "\n",
    "conv_9 = Conv2D(256, kernel_size=(3, 3), padding = 'same')(drop_8_2)\n",
    "batch_n_9 = BatchNormalization(axis = 3)(conv_9)\n",
    "leaky_9 = LeakyReLU()(batch_n_9)\n",
    "drop_9 = Dropout(0.25)(leaky_9)\n",
    "conv_9_2 = Conv2D(256, kernel_size=(3, 3), padding = 'same')(drop_9)\n",
    "res_9 = keras.layers.add([conv_9_2, drop_8_2])\n",
    "batch_n_9_2 = BatchNormalization(axis = 3)(res_9)\n",
    "leaky_9_2 = LeakyReLU()(batch_n_9_2)\n",
    "drop_9_2 = Dropout(0.25)(leaky_9_2)\n",
    "\n",
    "conv_10 = Conv2D(256, kernel_size=(3, 3), padding = 'same')(drop_9_2)\n",
    "batch_n_10 = BatchNormalization(axis = 3)(conv_10)\n",
    "leaky_10 = LeakyReLU()(batch_n_10)\n",
    "drop_10 = Dropout(0.25)(leaky_10)\n",
    "conv_10_2 = Conv2D(256, kernel_size=(3, 3), padding = 'same')(drop_10)\n",
    "res_10 = keras.layers.add([conv_10_2, drop_9_2])\n",
    "batch_n_10_2 = BatchNormalization(axis = 3)(res_10)\n",
    "leaky_10_2 = LeakyReLU()(batch_n_10_2)\n",
    "drop_10_2 = Dropout(0.25)(leaky_10_2)\n",
    "\n",
    "conv_11 = Conv2D(256, kernel_size=(3, 3), padding = 'same')(drop_10_2)\n",
    "batch_n_11 = BatchNormalization(axis = 3)(conv_11)\n",
    "leaky_11 = LeakyReLU()(batch_n_11)\n",
    "drop_11 = Dropout(0.25)(leaky_11)\n",
    "conv_11_2 = Conv2D(256, kernel_size=(3, 3), padding = 'same')(drop_11)\n",
    "res_11 = keras.layers.add([conv_11_2, drop_10_2])\n",
    "batch_n_11_2 = BatchNormalization(axis = 3)(res_11)\n",
    "leaky_11_2 = LeakyReLU()(batch_n_11_2)\n",
    "drop_11_2 = Dropout(0.25)(leaky_11_2)\n",
    "\n",
    "#conv_512 = Conv2D(512, kernel_size=(3, 3), padding = 'same', strides = 2)(batch_n_11_2)\n",
    "\n",
    "apool = keras.layers.AveragePooling2D(pool_size=(2, 2), padding='same')(drop_11_2)\n",
    "\n",
    "flat = Flatten()(apool)\n",
    "\n",
    "dense = Dense(1000, activation = 'softmax')(flat)\n",
    "model = Model(inputs=[x], outputs=[dense])\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\"\"\"batch_size = [40, 80, 160, 320, 480]\n",
    "for _ in range(len(batch_size)):\n",
    "    nums = np.array([1, 2, 3, 4])\n",
    "    np.random.shuffle(nums)\n",
    "    for train_num in nums:\n",
    "        print(_ + 1, '', 4)\n",
    "        imgs = np.load('data/imgs-' + str(train_num) + '.npy')\n",
    "        labels = np.load('data/labels-' + str(train_num) + '.npy')\n",
    "        model.fit(np.reshape(imgs, [len(imgs), 64, 64, 1]), labels, batch_size = batch_size[_], epochs = 1, verbose = 1)\n",
    "        imgs = np.empty([0])\n",
    "        labels = np.empty([0])\n",
    "\n",
    "    test = np.load('data/pretest.npy')\n",
    "    res = model.predict(np.reshape(test, [len(test), 64, 64, 1]))\n",
    "    test = np.empty([0])\n",
    "    saver('out' + str(6 + _) + '.csv', res)\n",
    "    res = np.empty([0])\n",
    "    model.save('my_model.h5')\"\"\"\n",
    "nums = np.array([1, 2, 3, 4])\n",
    "#np.random.shuffle(nums)\n",
    "for train_num in nums:\n",
    "    print(train_num, 'from', len(nums))\n",
    "    imgs = np.load('data/imgs-' + str(train_num) + '.npy')\n",
    "    imgs = np.reshape(imgs, [len(imgs), 64, 64, 1])\n",
    "    labels = np.load('data/labels-' + str(train_num) + '.npy')\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.03,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.03)  # randomly shift images vertically (fraction of total height)\n",
    "    datagen.fit(imgs)\n",
    "    model.fit_generator(datagen.flow(imgs, labels, batch_size=69),\n",
    "                        steps_per_epoch = (len(imgs) / 69), epochs=2, verbose=1)\n",
    "    datagen = 0\n",
    "    imgs = 0\n",
    "    labels = 0\n",
    "    if (train_num < 4):\n",
    "        imgs = np.load('data/imgs-' + str(4) + '.npy')[:20000]\n",
    "        imgs = np.reshape(imgs, [len(imgs), 64, 64, 1])\n",
    "        labels = np.load('data/labels-' + str(4) + '.npy')[:20000]\n",
    "        score = model.evaluate(imgs, labels, batch_size=100)\n",
    "        print(score)\n",
    "        imgs = 0\n",
    "        labels = 0\n",
    "    model.save('my_model.h5')\n",
    "test = np.load('data/pretest.npy')\n",
    "res = model.predict(np.reshape(test, [len(test), 64, 64, 1]))\n",
    "test = np.empty([0])\n",
    "saver('out' + str(6) + '.csv', res)\n",
    "res = np.empty([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 from 4\n",
      "Epoch 1/2\n",
      "347/346 [==============================] - 446s - loss: 0.1579 - acc: 0.9524   \n",
      "Epoch 2/2\n",
      "347/346 [==============================] - 443s - loss: 0.1296 - acc: 0.9597   \n",
      "2 from 4\n",
      "Epoch 1/2\n",
      "347/346 [==============================] - 443s - loss: 0.1573 - acc: 0.9536   \n",
      "Epoch 2/2\n",
      "347/346 [==============================] - 447s - loss: 0.1217 - acc: 0.9631   \n",
      "3 from 4\n",
      "Epoch 1/2\n",
      "347/346 [==============================] - 443s - loss: 0.1419 - acc: 0.9573   \n",
      "Epoch 2/2\n",
      "347/346 [==============================] - 444s - loss: 0.1140 - acc: 0.9651   \n",
      "1 from 4\n",
      "Epoch 1/2\n",
      "347/346 [==============================] - 445s - loss: 0.1052 - acc: 0.9684   \n",
      "Epoch 2/2\n",
      "347/346 [==============================] - 447s - loss: 0.0838 - acc: 0.9747   \n"
     ]
    }
   ],
   "source": [
    "batch_size = 240\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "nums = np.array([4, 2, 3, 1])\n",
    "for train_num in nums:\n",
    "    print(train_num, 'from', len(nums))\n",
    "    imgs = np.load('data/imgs-' + str(train_num) + '.npy')\n",
    "    imgs = np.reshape(imgs, [len(imgs), 64, 64, 1])\n",
    "    labels = np.load('data/labels-' + str(train_num) + '.npy')\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        rotation_range=4,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.03,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.03)  # randomly shift images vertically (fraction of total height)\n",
    "    datagen.fit(imgs)\n",
    "    model.fit_generator(datagen.flow(imgs, labels, batch_size=batch_size),\n",
    "                        steps_per_epoch = (len(imgs) / batch_size), epochs=2, verbose=1)\n",
    "    datagen = 0\n",
    "    imgs = 0\n",
    "    labels = 0\n",
    "    model.save('my_model1.h5')\n",
    "test = np.load('data/pretest.npy')\n",
    "res = model.predict(np.reshape(test, [len(test), 64, 64, 1]))\n",
    "test = np.empty([0])\n",
    "saver('out' + str(7) + '.csv', res)\n",
    "res = np.empty([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load('data/pretest.npy')\n",
    "res = model.predict(np.reshape(test, [len(test), 64, 64, 1]))\n",
    "test = np.empty([0])\n",
    "saver('out6.csv', res)\n",
    "res = np.empty([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.empty([0])\n",
    "labels = np.empty([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load('data/pretest.npy')\n",
    "res = model.predict(np.reshape(test, [len(test), 64, 64, 1]))\n",
    "test = np.empty([0])\n",
    "def saver(path, res):\n",
    "    retranslator = np.load('data/retranslator.npy')\n",
    "    with open(path, 'w') as out:\n",
    "        print('Id,Category', file=out)\n",
    "        for i in range(len(res)):\n",
    "            print('{i},{res}'.format(i = i + 1, res=int(retranslator[np.argmax(res[i])])), file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>64432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>64709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>64177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>61881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Category\n",
       "0   1     63955\n",
       "1   2     64432\n",
       "2   3     64709\n",
       "3   4     64177\n",
       "4   5     61881"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('out5.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83247, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64202 64202 64202 64202] 64202\n"
     ]
    }
   ],
   "source": [
    "res = np.empty((83247, 0), dtype = np.int64)\n",
    "for num in [1, 3, 5, 6]:\n",
    "    df = pd.read_csv('out' + str(num) + '.csv')\n",
    "    res_i = df['Category']\n",
    "    res = np.hstack((res, res_i.reshape((83247, 1))))\n",
    "print(res[100], np.bincount(res[100]).argmax())\n",
    "path = 'out4.csv'\n",
    "with open(path, 'w') as out:\n",
    "    print('Id,Category', file=out)\n",
    "    for i in range(len(res)):\n",
    "        print('{i},{res}'.format(i= i + 1, res=np.bincount(res[i]).argmax()), file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>64432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>64709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>64177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>61881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Category\n",
       "0   1     63955\n",
       "1   2     64432\n",
       "2   3     64709\n",
       "3   4     64177\n",
       "4   5     61881"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('out4.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61623 61623 61623] 61623\n"
     ]
    }
   ],
   "source": [
    "print(res[103], np.bincount(res[103]).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 62, 62, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 31, 31, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 29, 29, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 29, 29, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2200)              10139800  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 2200)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2200)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              2201000   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 12,628,128\n",
      "Trainable params: 12,627,680\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "1  3\n",
      "Epoch 1/1\n",
      "83246/83246 [==============================] - 610s - loss: 2.3705 - acc: 0.5803   \n",
      "1  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 607s - loss: 0.7679 - acc: 0.8067   \n",
      "1  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 633s - loss: 0.5080 - acc: 0.8633   \n",
      "1  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 619s - loss: 0.4570 - acc: 0.8767   \n",
      "2  3\n",
      "Epoch 1/1\n",
      "83246/83246 [==============================] - 546s - loss: 0.1881 - acc: 0.9488   \n",
      "2  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 537s - loss: 0.2263 - acc: 0.9388   \n",
      "2  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 518s - loss: 0.1963 - acc: 0.9446   \n",
      "2  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 524s - loss: 0.2145 - acc: 0.9390   \n",
      "3  3\n",
      "Epoch 1/1\n",
      "83246/83246 [==============================] - 481s - loss: 0.0947 - acc: 0.9736   \n",
      "3  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 469s - loss: 0.1159 - acc: 0.9674   \n",
      "3  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 468s - loss: 0.1041 - acc: 0.9698   \n",
      "3  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 466s - loss: 0.1171 - acc: 0.9656   \n",
      "4  3\n",
      "Epoch 1/1\n",
      "83246/83246 [==============================] - 450s - loss: 0.0615 - acc: 0.9828   \n",
      "4  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 451s - loss: 0.0770 - acc: 0.9782   \n",
      "4  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 456s - loss: 0.0699 - acc: 0.9795   \n",
      "4  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 449s - loss: 0.0810 - acc: 0.9752   \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, LeakyReLU, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(64, 64, 1)))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2200))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "batch_size = [40, 80, 140, 200]\n",
    "for _ in range(4):\n",
    "    for train_num in range(1, 5):\n",
    "        print(_ + 1, '', 3)\n",
    "        imgs = np.load('data/imgs-' + str(train_num) + '.npy')\n",
    "        labels = np.load('data/labels-' + str(train_num) + '.npy')\n",
    "        model.fit(np.reshape(imgs, [len(imgs), 64, 64, 1]), labels, batch_size = batch_size[_], epochs = 1, verbose = 1)\n",
    "        imgs = np.empty([0])\n",
    "        labels = np.empty([0])\n",
    "test = np.load('data/pretest.npy')\n",
    "res = model.predict(np.reshape(test, [len(test), 64, 64, 1]))\n",
    "test = np.empty([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 439s - loss: 0.0549 - acc: 0.9829   \n",
      "5  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 445s - loss: 0.0683 - acc: 0.9795   \n",
      "5  3\n",
      "Epoch 1/1\n",
      "83247/83247 [==============================] - 451s - loss: 0.0812 - acc: 0.9755   \n",
      "5  3\n",
      "Epoch 1/1\n",
      "83246/83246 [==============================] - 453s - loss: 0.0579 - acc: 0.9838   \n"
     ]
    }
   ],
   "source": [
    "for train_num in range(4, 0, -1):\n",
    "    print(5, '', 3)\n",
    "    imgs = np.load('data/imgs-' + str(train_num) + '.npy')\n",
    "    labels = np.load('data/labels-' + str(train_num) + '.npy')\n",
    "    model.fit(np.reshape(imgs, [len(imgs), 64, 64, 1]), labels, batch_size = 200, epochs = 1, verbose = 1)\n",
    "    imgs = np.empty([0])\n",
    "    labels = np.empty([0])\n",
    "\n",
    "def saver(path, res):\n",
    "    retranslator = np.load('data/retranslator.npy')\n",
    "    with open(path, 'w') as out:\n",
    "        print('Id,Category', file=out)\n",
    "        for i in range(len(res)):\n",
    "            print('{i},{res}'.format(i = i + 1, res=int(retranslator[np.argmax(res[i])])), file=out)\n",
    "\n",
    "#res = np.array([retranslator[np.argmax(elem)] for elem in res])\n",
    "saver('out7.csv', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load('data/pretest.npy')\n",
    "res = model.predict(np.reshape(test, [len(test), 64, 64, 1]))\n",
    "test = np.empty([0])\n",
    "\n",
    "def saver(path, res):\n",
    "    retranslator = np.load('data/retranslator.npy')\n",
    "    with open(path, 'w') as out:\n",
    "        print('Id,Category', file=out)\n",
    "        for i in range(len(res)):\n",
    "            print('{i},{res}'.format(i = i + 1, res=int(retranslator[np.argmax(res[i])])), file=out)\n",
    "\n",
    "#res = np.array([retranslator[np.argmax(elem)] for elem in res])\n",
    "saver('out7.csv', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:115: UserWarning: Output \"custom_variational_layer_2\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_2\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 64, 64, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 64, 64, 1)     5           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 32, 32, 64)    320         conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 32, 32, 64)    36928       conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 32, 32, 64)    36928       conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 65536)         0           conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 2048)          134219776   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 2)             4098        dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 2)             4098        dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 2)             0           dense_7[0][0]                    \n",
      "                                                                   dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 2048)          6144        lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 12544)         25702656    dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 14, 14, 64)    0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTransp (None, 14, 14, 64)    36928       reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTransp (None, 14, 14, 64)    36928       conv2d_transpose_4[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTransp (None, 29, 29, 64)    36928       conv2d_transpose_5[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 28, 28, 1)     257         conv2d_transpose_6[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "custom_variational_layer_2 (Cust [(None, 64, 64, 1), ( 0           input_2[0][0]                    \n",
      "                                                                   conv2d_10[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 160,121,994\n",
      "Trainable params: 160,121,994\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "1  3\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64]\n\t [[Node: conv2d_8/bias/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@conv2d_8/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_8/bias, conv2d_2/Const)]]\n\nCaused by op 'conv2d_8/bias/Assign', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-69759a2e4ce1>\", line 36, in <module>\n    strides=1)(conv_2)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 575, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/convolutional.py\", line 140, in build\n    constraint=self.bias_constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 399, in add_weight\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 316, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 346, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 57, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64]\n\t [[Node: conv2d_8/bias/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@conv2d_8/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_8/bias, conv2d_2/Const)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64]\n\t [[Node: conv2d_8/bias/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@conv2d_8/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_8/bias, conv2d_2/Const)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-69759a2e4ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/imgs-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2269\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2270\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64]\n\t [[Node: conv2d_8/bias/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@conv2d_8/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_8/bias, conv2d_2/Const)]]\n\nCaused by op 'conv2d_8/bias/Assign', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-69759a2e4ce1>\", line 36, in <module>\n    strides=1)(conv_2)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 575, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/convolutional.py\", line 140, in build\n    constraint=self.bias_constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 399, in add_weight\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 316, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 346, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 57, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64]\n\t [[Node: conv2d_8/bias/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@conv2d_8/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_8/bias, conv2d_2/Const)]]\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=(64, 64, 1))\n",
    "\n",
    "conv_1 = Conv2D(32, kernel_size=(3, 3))(x)\n",
    "batch_n_1 = BatchNormalization(axis = 3)(conv_1)\n",
    "leaky_1 = LeakyReLU()(batch_n_1)\n",
    "conv_1_2 = Conv2D(32, kernel_size=(3, 3))(leaky_1)\n",
    "res_1 = keras.layers.concatenate([conv_1_2, x])\n",
    "batch_n_1_2 = BatchNormalization(axis = 3)(res_1)\n",
    "\n",
    "conv_2 = Conv2D(32, kernel_size=(3, 3))(batch_n_1_2)\n",
    "batch_n_2 = BatchNormalization(axis = 3)(conv_2)\n",
    "leaky_2 = LeakyReLU()(batch_n_2)\n",
    "conv_2_2 = Conv2D(32, kernel_size=(3, 3))(leaky_2)\n",
    "res_2 = keras.layers.concatenate([conv_2_2, batch_n_1_2])\n",
    "batch_n_2_2 = BatchNormalization(axis = 3)(res_2)\n",
    "\n",
    "conv_3 = Conv2D(32, kernel_size=(3, 3))(batch_n_2_2)\n",
    "batch_n_3 = BatchNormalization(axis = 3)(conv_3)\n",
    "leaky_3 = LeakyReLU()(batch_n_3)\n",
    "conv_3_2 = Conv2D(32, kernel_size=(3, 3))(leaky_3)\n",
    "res_3 = keras.layers.concatenate([conv_3_2, batch_n_2_2])\n",
    "batch_n_3_2 = BatchNormalization(axis = 3)(res_3)\n",
    "\n",
    "flat = Flatten()(batch_n_3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
